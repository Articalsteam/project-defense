{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fffb8cd",
   "metadata": {},
   "source": [
    "# Supply Chain Delay Prediction System\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting supply chain delays using ensemble methods and advanced feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e8034",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries for data manipulation, model building, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c173dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspaces/project-defense/supply_chain_prediction')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader import SupplyChainDataGenerator\n",
    "from feature_engineering import FeatureEngineer\n",
    "from models import DelayPredictionModel, EnsembleDelayPredictor\n",
    "from evaluation import ModelEvaluator, DelayAnalyzer\n",
    "from visualization import PredictionVisualizer\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c1101",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Supply Chain Data\n",
    "\n",
    "Generate synthetic supply chain data and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic supply chain data\n",
    "generator = SupplyChainDataGenerator(random_state=42)\n",
    "df = generator.generate_dataset(n_samples=1000)\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a558ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Distribution of delay target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['delay_days'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Delay (days)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Actual Delays')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(['On-time', 'Delayed'], \n",
    "            [np.sum(df['delay_days'] <= 0), np.sum(df['delay_days'] > 0)],\n",
    "            color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('On-time vs Delayed Shipments')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDelay Summary:\")\n",
    "print(f\"  Mean delay: {df['delay_days'].mean():.2f} days\")\n",
    "print(f\"  Median delay: {df['delay_days'].median():.2f} days\")\n",
    "print(f\"  Max delay: {df['delay_days'].max():.2f} days\")\n",
    "print(f\"  % Delayed: {np.sum(df['delay_days'] > 0) / len(df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze delays by key categories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# By product category\n",
    "df.groupby('product_category')['delay_days'].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Average Delay by Product Category')\n",
    "axes[0, 0].set_ylabel('Delay (days)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By transportation mode\n",
    "df.groupby('transportation_mode')['delay_days'].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar', ax=axes[0, 1], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Average Delay by Transportation Mode')\n",
    "axes[0, 1].set_ylabel('Delay (days)')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By weather condition\n",
    "df.groupby('weather_condition')['delay_days'].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar', ax=axes[1, 0], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Average Delay by Weather Condition')\n",
    "axes[1, 0].set_ylabel('Delay (days)')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Correlation with delay\n",
    "correlation_data = df[['order_quantity', 'order_value', 'supplier_reliability_score',\n",
    "                       'distance_km', 'fuel_price_index', 'port_congestion_score',\n",
    "                       'customs_clearance_hours', 'scheduled_delivery_days',\n",
    "                       'historical_delay_rate', 'supplier_inventory_level', 'delay_days']].corr()['delay_days'].sort_values(ascending=False)\n",
    "\n",
    "correlation_data.drop('delay_days').plot(kind='barh', ax=axes[1, 1], color='purple', edgecolor='black')\n",
    "axes[1, 1].set_title('Feature Correlation with Delay')\n",
    "axes[1, 1].set_xlabel('Correlation Coefficient')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c91f21",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Handle categorical variables, normalize numerical features, and create interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85faac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Add temporal features\n",
    "df_processed = engineer.add_temporal_features(df.copy())\n",
    "\n",
    "# Add interaction features\n",
    "df_processed = engineer.add_interaction_features(df_processed)\n",
    "\n",
    "print(\"Features before engineering:\", len(df.columns))\n",
    "print(\"Features after engineering:\", len(df_processed.columns))\n",
    "\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = set(df_processed.columns) - set(df.columns)\n",
    "for feature in sorted(new_features):\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Show sample of engineered features\n",
    "print(\"\\nSample of engineered data:\")\n",
    "print(df_processed[['date', 'day_of_week', 'month', 'is_weekend', 'estimated_transit_hours', \n",
    "                     'value_per_unit', 'reliability_consistency']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab28e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features (encoding and scaling)\n",
    "X, feature_names = engineer.fit_transform(df_processed)\n",
    "y = df_processed['delay_days'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features: {len(feature_names)}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(feature_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "\n",
    "# Display feature statistics after scaling\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "print(\"\\nFeature statistics after scaling (first 10 features):\")\n",
    "print(X_df.iloc[:, :10].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdef49a",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "Split the dataset into training and testing sets using temporal ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496637c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data temporally (sorted by date)\n",
    "df_sorted = df_processed.sort_values('date').reset_index(drop=True)\n",
    "X_sorted = X[df_sorted.index]\n",
    "y_sorted = y[df_sorted.index]\n",
    "\n",
    "# Temporal split: 70% train, 15% validation, 15% test\n",
    "n_total = len(X_sorted)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "X_train = X_sorted[:n_train]\n",
    "y_train = y_sorted[:n_train]\n",
    "\n",
    "X_val = X_sorted[n_train:n_train + n_val]\n",
    "y_val = y_sorted[n_train:n_train + n_val]\n",
    "\n",
    "X_test = X_sorted[n_train + n_val:]\n",
    "y_test = y_sorted[n_train + n_val:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/n_total*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} ({len(X_val)/n_total*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/n_total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining target statistics:\")\n",
    "print(f\"  Mean: {y_train.mean():.2f} days\")\n",
    "print(f\"  Std: {y_train.std():.2f} days\")\n",
    "print(f\"  % Delayed: {np.sum(y_train > 0) / len(y_train) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53363307",
   "metadata": {},
   "source": [
    "## 5. Build Baseline Model\n",
    "\n",
    "Create a simple baseline model to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline linear regression model\n",
    "print(\"Training baseline model (Linear Regression)...\\n\")\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_baseline = baseline_model.predict(X_train)\n",
    "y_val_pred_baseline = baseline_model.predict(X_val)\n",
    "y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "train_metrics_baseline = evaluator.calculate_metrics(y_train, y_train_pred_baseline)\n",
    "val_metrics_baseline = evaluator.calculate_metrics(y_val, y_val_pred_baseline)\n",
    "test_metrics_baseline = evaluator.calculate_metrics(y_test, y_test_pred_baseline)\n",
    "\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for key, value in train_metrics_baseline.items():\n",
    "    print(f\"  {key.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for key, value in val_metrics_baseline.items():\n",
    "    print(f\"  {key.upper()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for key, value in test_metrics_baseline.items():\n",
    "    print(f\"  {key.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5055a",
   "metadata": {},
   "source": [
    "## 6. Train Advanced Models\n",
    "\n",
    "Train multiple advanced machine learning models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train individual models\n",
    "print(\"Training advanced models...\\n\")\n",
    "\n",
    "models_dict = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.05, random_state=42, verbose=0),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.05, random_state=42, verbose=-1),\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models_dict.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[model_name] = model\n",
    "    \n",
    "    # Predictions on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluator.calculate_metrics(y_val, y_val_pred)\n",
    "    model_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"  ✓ Complete - Val R²: {metrics['r2']:.4f}, Val RMSE: {metrics['rmse']:.4f}\\n\")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble model\n",
    "print(\"Training Ensemble Model...\")\n",
    "\n",
    "ensemble = EnsembleDelayPredictor(['xgboost', 'lightgbm', 'random_forest'])\n",
    "ensemble.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "y_val_pred_ensemble = ensemble.predict(X_val)\n",
    "metrics_ensemble = evaluator.calculate_metrics(y_val, y_val_pred_ensemble)\n",
    "model_results['Ensemble'] = metrics_ensemble\n",
    "\n",
    "print(f\"  ✓ Complete - Val R²: {metrics_ensemble['r2']:.4f}, Val RMSE: {metrics_ensemble['rmse']:.4f}\\n\")\n",
    "\n",
    "# Store best model\n",
    "best_model_name = max(model_results, key=lambda x: model_results[x]['r2'])\n",
    "print(f\"\\nBest model on validation set: {best_model_name}\")\n",
    "print(f\"  R² Score: {model_results[best_model_name]['r2']:.4f}\")\n",
    "print(f\"  RMSE: {model_results[best_model_name]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891c224",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Comparison\n",
    "\n",
    "Evaluate all models using comprehensive metrics and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec37b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Baseline model\n",
    "y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "test_results['Linear Regression (Baseline)'] = evaluator.calculate_metrics(y_test, y_test_pred_baseline)\n",
    "\n",
    "# Individual models\n",
    "for model_name, model in trained_models.items():\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_results[model_name] = evaluator.calculate_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Ensemble\n",
    "y_test_pred_ensemble = ensemble.predict(X_test)\n",
    "test_results['Ensemble'] = evaluator.calculate_metrics(y_test, y_test_pred_ensemble)\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(test_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(results_df.sort_values('r2', ascending=False))\n",
    "\n",
    "# Find best model\n",
    "best_model = results_df['r2'].idxmax()\n",
    "print(f\"\\n✓ Best Model: {best_model}\")\n",
    "print(f\"  R² Score: {results_df.loc[best_model, 'r2']:.4f}\")\n",
    "print(f\"  RMSE: {results_df.loc[best_model, 'rmse']:.4f}\")\n",
    "print(f\"  MAE: {results_df.loc[best_model, 'mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "PredictionVisualizer.plot_metrics_comparison(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129db118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best model (Ensemble)\n",
    "print(\"\\nDETAILED ANALYSIS OF BEST MODEL (ENSEMBLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification metrics (delayed vs on-time)\n",
    "class_metrics = evaluator.calculate_classification_metrics(y_test, y_test_pred_ensemble, threshold=0.5)\n",
    "\n",
    "print(\"\\nClassification Metrics (Delayed vs On-time):\")\n",
    "for key, value in class_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Residual analysis\n",
    "residual_stats = evaluator.get_residual_statistics(y_test, y_test_pred_ensemble)\n",
    "\n",
    "print(\"\\nResidual Statistics:\")\n",
    "for key, value in residual_stats.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Visualizations\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "PredictionVisualizer.plot_predictions_vs_actual(y_test, y_test_pred_ensemble, \"Ensemble Model: Actual vs Predicted Delays\")\n",
    "PredictionVisualizer.plot_error_distribution(y_test, y_test_pred_ensemble, \"Ensemble Model: Prediction Error Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1be56",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "Analyze which features have the greatest impact on delay predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee702c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from different models\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = trained_models['XGBoost']\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "top_indices = np.argsort(xgb_importance)[-15:]\n",
    "axes[0, 0].barh(range(len(top_indices)), xgb_importance[top_indices], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_yticks(range(len(top_indices)))\n",
    "axes[0, 0].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "axes[0, 0].set_title('XGBoost - Top 15 Features')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = trained_models['LightGBM']\n",
    "lgb_importance = lgb_model.feature_importances_\n",
    "top_indices = np.argsort(lgb_importance)[-15:]\n",
    "axes[0, 1].barh(range(len(top_indices)), lgb_importance[top_indices], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_yticks(range(len(top_indices)))\n",
    "axes[0, 1].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "axes[0, 1].set_xlabel('Importance')\n",
    "axes[0, 1].set_title('LightGBM - Top 15 Features')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "rf_importance = rf_model.feature_importances_\n",
    "top_indices = np.argsort(rf_importance)[-15:]\n",
    "axes[1, 0].barh(range(len(top_indices)), rf_importance[top_indices], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_yticks(range(len(top_indices)))\n",
    "axes[1, 0].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "axes[1, 0].set_xlabel('Importance')\n",
    "axes[1, 0].set_title('Random Forest - Top 15 Features')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# Average importance across models\n",
    "avg_importance = (np.array(xgb_importance) + np.array(lgb_importance) + np.array(rf_importance)) / 3\n",
    "top_indices = np.argsort(avg_importance)[-15:]\n",
    "axes[1, 1].barh(range(len(top_indices)), avg_importance[top_indices], color='gold', edgecolor='black')\n",
    "axes[1, 1].set_yticks(range(len(top_indices)))\n",
    "axes[1, 1].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "axes[1, 1].set_xlabel('Importance')\n",
    "axes[1, 1].set_title('Average Importance - Top 15 Features')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 10 features\n",
    "print(\"\\nTop 10 Most Important Features (by average):\")\n",
    "sorted_indices = np.argsort(avg_importance)[::-1]\n",
    "for rank, idx in enumerate(sorted_indices[:10], 1):\n",
    "    print(f\"  {rank}. {feature_names[idx]}: {avg_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92121929",
   "metadata": {},
   "source": [
    "## 9. Make Predictions on New Data\n",
    "\n",
    "Use the best-performing model to make predictions and generate actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4eaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on entire test set\n",
    "predictions = ensemble.predict(X_test)\n",
    "uncertainty = ensemble.predict_with_uncertainty(X_test)[1]\n",
    "\n",
    "# Create results dataframe\n",
    "test_data = df_sorted[n_train + n_val:].reset_index(drop=True)\n",
    "results_df = test_data.copy()\n",
    "results_df['predicted_delay_days'] = predictions\n",
    "results_df['uncertainty'] = uncertainty\n",
    "results_df['confidence_lower'] = predictions - 1.96 * uncertainty\n",
    "results_df['confidence_upper'] = predictions + 1.96 * uncertainty\n",
    "results_df['is_delayed'] = (predictions > 0).astype(int)\n",
    "results_df['risk_level'] = pd.cut(\n",
    "    predictions,\n",
    "    bins=[0, 1, 3, 5, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High', 'Critical']\n",
    ")\n",
    "\n",
    "print(\"PREDICTIONS ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal test shipments: {len(results_df)}\")\n",
    "\n",
    "# Risk distribution\n",
    "print(\"\\nRisk Level Distribution:\")\n",
    "risk_counts = results_df['risk_level'].value_counts()\n",
    "for risk in ['Low', 'Medium', 'High', 'Critical']:\n",
    "    count = risk_counts.get(risk, 0)\n",
    "    pct = count / len(results_df) * 100\n",
    "    print(f\"  {risk}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show high-risk shipments\n",
    "analyzer = DelayAnalyzer()\n",
    "high_risk = analyzer.identify_high_risk_shipments(test_data, predictions, threshold=5.0)\n",
    "\n",
    "print(f\"\\nHigh-Risk Shipments (predicted delay > 5 days): {len(high_risk)}\")\n",
    "print(\"\\nTop 10 High-Risk Shipments:\")\n",
    "high_risk_display = high_risk[['supplier_id', 'warehouse_id', 'product_category', \n",
    "                               'transportation_mode', 'predicted_delay', 'risk_level']].head(10)\n",
    "print(high_risk_display.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actionable insights\n",
    "print(\"\\nACTIONABLE INSIGHTS FROM PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analysis by category\n",
    "print(\"\\nAverage Predicted Delay by Product Category:\")\n",
    "category_delays = results_df.groupby('product_category')['predicted_delay_days'].agg(['mean', 'count'])\n",
    "print(category_delays.round(2))\n",
    "\n",
    "# Analysis by transportation mode\n",
    "print(\"\\nAverage Predicted Delay by Transportation Mode:\")\n",
    "transport_delays = results_df.groupby('transportation_mode')['predicted_delay_days'].agg(['mean', 'count'])\n",
    "print(transport_delays.round(2))\n",
    "\n",
    "# Most problematic suppliers\n",
    "print(\"\\nTop 5 Suppliers with Highest Average Predicted Delays:\")\n",
    "supplier_delays = results_df.groupby('supplier_id')['predicted_delay_days'].mean().sort_values(ascending=False).head(5)\n",
    "for supplier_id, delay in supplier_delays.items():\n",
    "    print(f\"  Supplier {supplier_id}: {delay:.2f} days average delay\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# By category\n",
    "results_df.boxplot(column='predicted_delay_days', by='product_category', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Predicted Delays by Product Category')\n",
    "axes[0, 0].set_xlabel('Category')\n",
    "axes[0, 0].set_ylabel('Delay (days)')\n",
    "\n",
    "# By transportation\n",
    "results_df.boxplot(column='predicted_delay_days', by='transportation_mode', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Predicted Delays by Transportation Mode')\n",
    "axes[0, 1].set_xlabel('Mode')\n",
    "axes[0, 1].set_ylabel('Delay (days)')\n",
    "\n",
    "# Risk distribution\n",
    "risk_dist = results_df['risk_level'].value_counts()\n",
    "axes[1, 0].bar(risk_dist.index, risk_dist.values, color=['green', 'yellow', 'orange', 'red'], edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of Risk Levels')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Delay distribution\n",
    "axes[1, 1].hist(results_df['predicted_delay_days'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1, 1].axvline(results_df['predicted_delay_days'].mean(), color='r', linestyle='--', linewidth=2, label=f'Mean: {results_df[\"predicted_delay_days\"].mean():.2f}')\n",
    "axes[1, 1].set_title('Distribution of Predicted Delays')\n",
    "axes[1, 1].set_xlabel('Delay (days)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4990e3",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Preparation\n",
    "\n",
    "Save the model and prepare it for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the ensemble model\n",
    "model_path = '/workspaces/project-defense/supply_chain_prediction/ensemble_model.pkl'\n",
    "engineer_path = '/workspaces/project-defense/supply_chain_prediction/feature_engineer.pkl'\n",
    "\n",
    "joblib.dump(ensemble, model_path)\n",
    "joblib.dump(engineer, engineer_path)\n",
    "\n",
    "print(\"MODEL DEPLOYMENT PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n✓ Ensemble model saved to: {model_path}\")\n",
    "print(f\"✓ Feature engineer saved to: {engineer_path}\")\n",
    "\n",
    "# Create a deployment summary\n",
    "deployment_summary = {\n",
    "    'model_type': 'Ensemble (XGBoost, LightGBM, Random Forest)',\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': feature_names,\n",
    "    'performance_metrics': {\n",
    "        'test_r2_score': float(test_results['Ensemble']['r2']),\n",
    "        'test_rmse': float(test_results['Ensemble']['rmse']),\n",
    "        'test_mae': float(test_results['Ensemble']['mae']),\n",
    "        'test_mape': float(test_results['Ensemble']['mape']),\n",
    "    },\n",
    "    'data_info': {\n",
    "        'total_features': len(feature_names),\n",
    "        'numerical_features': len(engineer.numerical_features),\n",
    "        'categorical_features': len(engineer.categorical_features),\n",
    "    },\n",
    "    'model_assumptions': [\n",
    "        'Temporal data split used (no data leakage)',\n",
    "        'Features scaled using StandardScaler',\n",
    "        'Categorical variables label-encoded',\n",
    "        'Missing values handled during data generation',\n",
    "    ],\n",
    "    'deployment_notes': [\n",
    "        'Model requires feature engineer for transformation',\n",
    "        'Input features must match training features in order',\n",
    "        'For optimal performance, ensure data quality matches training distribution',\n",
    "        'Regular monitoring recommended for data drift',\n",
    "        'Retrain model quarterly or when performance degrades',\n",
    "    ]\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "summary_path = '/workspaces/project-defense/supply_chain_prediction/model_deployment_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(deployment_summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Deployment summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\nDEPLOYMENT SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Model Type: {deployment_summary['model_type']}\")\n",
    "print(f\"\\nPerformance Metrics (Test Set):\")\n",
    "for metric, value in deployment_summary['performance_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nData Configuration:\")\n",
    "for key, value in deployment_summary['data_info'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel Assumptions:\")\n",
    "for assumption in deployment_summary['model_assumptions']:\n",
    "    print(f\"  • {assumption}\")\n",
    "\n",
    "print(f\"\\nDeployment Notes:\")\n",
    "for note in deployment_summary['deployment_notes']:\n",
    "    print(f\"  • {note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUPPLY CHAIN DELAY PREDICTION SYSTEM - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSUMMARY OF RESULTS:\")\n",
    "print(f\"  • Best Model: Ensemble (XGBoost + LightGBM + Random Forest)\")\n",
    "print(f\"  • Test R² Score: {test_results['Ensemble']['r2']:.4f}\")\n",
    "print(f\"  • Test RMSE: {test_results['Ensemble']['rmse']:.4f} days\")\n",
    "print(f\"  • Test MAE: {test_results['Ensemble']['mae']:.4f} days\")\n",
    "\n",
    "print(f\"\\nPREDICTION STATISTICS:\")\n",
    "print(f\"  • Total Test Shipments: {len(results_df)}\")\n",
    "print(f\"  • High-Risk Shipments: {len(high_risk)}\")\n",
    "print(f\"  • Average Predicted Delay: {predictions.mean():.2f} days\")\n",
    "print(f\"  • Max Predicted Delay: {predictions.max():.2f} days\")\n",
    "\n",
    "print(f\"\\nMODELS TRAINED: {len(models_dict) + 1}\")\n",
    "for model_name in test_results.keys():\n",
    "    r2 = test_results[model_name]['r2']\n",
    "    print(f\"  • {model_name}: R² = {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nFEATURES ENGINEERED: {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\nMODEL ARTIFACTS SAVED:\")\n",
    "print(f\"  • Model: {model_path}\")\n",
    "print(f\"  • Feature Engineer: {engineer_path}\")\n",
    "print(f\"  • Deployment Summary: {summary_path}\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Monitor model performance in production\")\n",
    "print(\"  2. Set up automated retraining pipeline\")\n",
    "print(\"  3. Establish alerts for high-risk shipments\")\n",
    "print(\"  4. Integrate with supply chain management system\")\n",
    "print(\"  5. Collect feedback and improve model periodically\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
